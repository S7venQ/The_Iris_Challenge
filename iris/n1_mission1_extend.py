# classifier_comparison.py
"""
æ‰©å±•åŠŸèƒ½ï¼šå¯è§†åŒ–ä¸åŒåˆ†ç±»å™¨åœ¨ä¸‰åˆ†ç±»/ä¸¤ä¸ªç‰¹å¾ä¸Šçš„ç»“æœ
æ¯”è¾ƒå¤šç§æœºå™¨å­¦ä¹ åˆ†ç±»å™¨åœ¨Irisæ•°æ®é›†ä¸Šçš„å†³ç­–è¾¹ç•Œå’Œæ€§èƒ½
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# å¯¼å…¥å¤šç§åˆ†ç±»å™¨
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœéœ€è¦æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_and_prepare_data():
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
    # åŠ è½½Irisæ•°æ®é›†
    iris = load_iris()
    
    # é€‰æ‹©åä¸¤ä¸ªç‰¹å¾ï¼ˆèŠ±ç“£é•¿åº¦å’Œå®½åº¦ï¼‰ç”¨äº2Då¯è§†åŒ–
    X = iris.data[:, 2:]  # åªä½¿ç”¨æœ€åä¸¤ä¸ªç‰¹å¾
    y = iris.target
    feature_names = iris.feature_names[2:]
    target_names = iris.target_names
    
    print("æ•°æ®é›†ä¿¡æ¯:")
    print(f"ç‰¹å¾å½¢çŠ¶: {X.shape}")
    print(f"ç‰¹å¾å: {feature_names}")
    print(f"ç±»åˆ«å: {target_names}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'X_train_scaled': X_train_scaled,
        'X_test_scaled': X_test_scaled,
        'X_full': X,
        'y_full': y,
        'feature_names': feature_names,
        'target_names': target_names,
        'scaler': scaler
    }

def create_classifiers():
    """åˆ›å»ºè¦æ¯”è¾ƒçš„åˆ†ç±»å™¨åˆ—è¡¨"""
    classifiers = [
        ('é€»è¾‘å›å½’', LogisticRegression(max_iter=1000, random_state=42)),
        ('çº¿æ€§SVM', SVC(kernel='linear', C=1.0, probability=True, random_state=42)),
        ('RBF SVM', SVC(kernel='rbf', gamma=0.7, C=1.0, probability=True, random_state=42)),
        ('Kè¿‘é‚» (k=5)', KNeighborsClassifier(n_neighbors=5)),
        ('å†³ç­–æ ‘', DecisionTreeClassifier(max_depth=4, random_state=42)),
        ('éšæœºæ£®æ—', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('é«˜æ–¯æœ´ç´ è´å¶æ–¯', GaussianNB()),
        ('ç¥ç»ç½‘ç»œ', MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42))
    ]
    return classifiers

def train_and_evaluate_classifiers(classifiers, data):
    """è®­ç»ƒå¹¶è¯„ä¼°æ‰€æœ‰åˆ†ç±»å™¨"""
    results = {}
    
    print("\n" + "="*60)
    print("åˆ†ç±»å™¨æ€§èƒ½è¯„ä¼°")
    print("="*60)
    
    for name, clf in classifiers:
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        clf.fit(data['X_train_scaled'], data['y_train'])
        
        # é¢„æµ‹
        y_pred = clf.predict(data['X_test_scaled'])
        y_pred_train = clf.predict(data['X_train_scaled'])
        
        # è®¡ç®—å‡†ç¡®ç‡
        test_acc = accuracy_score(data['y_test'], y_pred)
        train_acc = accuracy_score(data['y_train'], y_pred_train)
        
        # ä¿å­˜ç»“æœ
        results[name] = {
            'classifier': clf,
            'test_accuracy': test_acc,
            'train_accuracy': train_acc,
            'y_pred': y_pred,
            'confusion_matrix': confusion_matrix(data['y_test'], y_pred)
        }
        
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"  æ˜¯å¦è¿‡æ‹Ÿåˆ: {'æ˜¯' if train_acc - test_acc > 0.1 else 'å¦'}")
    
    return results

def plot_decision_boundaries(classifiers_results, data):
    """ç»˜åˆ¶æ‰€æœ‰åˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œ"""
    # åˆ›å»ºç½‘æ ¼ç”¨äºç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    X = data['X_full']
    h = 0.02  # ç½‘æ ¼æ­¥é•¿
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # è®¾ç½®é¢œè‰²
    colors = ['#FFAAAA', '#AAFFAA', '#AAAAFF']  # ä¸‰ç§ç±»åˆ«çš„é¢œè‰²
    cmap_light = mcolors.ListedColormap(['#FFCCCC', '#CCFFCC', '#CCCCFF'])
    cmap_bold = mcolors.ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    # è®¡ç®—å­å›¾çš„è¡Œåˆ—æ•°
    n_classifiers = len(classifiers_results)
    n_cols = 4
    n_rows = int(np.ceil(n_classifiers / n_cols))
    
    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(16, 4 * n_rows))
    
    for idx, (name, result) in enumerate(classifiers_results.items(), 1):
        clf = result['classifier']
        accuracy = result['test_accuracy']
        
        # æ ‡å‡†åŒ–ç½‘æ ¼ç‚¹
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        grid_points_scaled = data['scaler'].transform(grid_points)
        
        # é¢„æµ‹ç½‘æ ¼ç‚¹çš„ç±»åˆ«
        if hasattr(clf, "predict_proba"):
            Z = clf.predict_proba(grid_points_scaled)
            Z = np.argmax(Z, axis=1)
        else:
            Z = clf.predict(grid_points_scaled)
        
        Z = Z.reshape(xx.shape)
        
        # åˆ›å»ºå­å›¾
        ax = plt.subplot(n_rows, n_cols, idx)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)
        
        # ç»˜åˆ¶è®­ç»ƒæ•°æ®ç‚¹
        scatter = ax.scatter(data['X_train'][:, 0], data['X_train'][:, 1], 
                            c=data['y_train'], cmap=cmap_bold, 
                            edgecolor='black', s=50, alpha=0.8)
        
        # è®¾ç½®å›¾å½¢å±æ€§
        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_title(f'{name}\nå‡†ç¡®ç‡: {accuracy:.3f}', fontsize=10)
        ax.set_xlabel(data['feature_names'][0])
        ax.set_ylabel(data['feature_names'][1])
        
        # åªåœ¨ç¬¬ä¸€åˆ—æ·»åŠ yè½´æ ‡ç­¾
        if idx % n_cols != 1:
            ax.set_ylabel('')
    
    plt.tight_layout()
    plt.savefig('classifier_comparison_decision_boundaries.png', dpi=150, bbox_inches='tight')
    plt.show()

def plot_performance_comparison(results, data):
    """ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. å‡†ç¡®ç‡æ¯”è¾ƒæ¡å½¢å›¾
    ax1 = axes[0, 0]
    names = list(results.keys())
    test_accs = [results[name]['test_accuracy'] for name in names]
    train_accs = [results[name]['train_accuracy'] for name in names]
    
    x = np.arange(len(names))
    width = 0.35
    
    ax1.bar(x - width/2, train_accs, width, label='è®­ç»ƒå‡†ç¡®ç‡', alpha=0.8, color='skyblue')
    ax1.bar(x + width/2, test_accs, width, label='æµ‹è¯•å‡†ç¡®ç‡', alpha=0.8, color='lightcoral')
    
    ax1.set_xlabel('åˆ†ç±»å™¨')
    ax1.set_ylabel('å‡†ç¡®ç‡')
    ax1.set_title('è®­ç»ƒé›† vs æµ‹è¯•é›†å‡†ç¡®ç‡æ¯”è¾ƒ')
    ax1.set_xticks(x)
    ax1.set_xticklabels(names, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ å‡†ç¡®ç‡æ•°å€¼æ ‡ç­¾
    for i, (train, test) in enumerate(zip(train_accs, test_accs)):
        ax1.text(i - width/2, train + 0.01, f'{train:.3f}', ha='center', va='bottom', fontsize=8)
        ax1.text(i + width/2, test + 0.01, f'{test:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 2. è¿‡æ‹Ÿåˆç¨‹åº¦é›·è¾¾å›¾
    ax2 = axes[0, 1]
    overfit_scores = [train_accs[i] - test_accs[i] for i in range(len(names))]
    
    angles = np.linspace(0, 2 * np.pi, len(names), endpoint=False).tolist()
    overfit_scores += overfit_scores[:1]  # é—­åˆå›¾å½¢
    angles += angles[:1]  # é—­åˆè§’åº¦
    
    ax2 = plt.subplot(2, 2, 2, projection='polar')
    ax2.plot(angles, overfit_scores, 'o-', linewidth=2)
    ax2.fill(angles, overfit_scores, alpha=0.25)
    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(names)
    ax2.set_title('è¿‡æ‹Ÿåˆç¨‹åº¦é›·è¾¾å›¾\n(è®­ç»ƒå‡†ç¡®ç‡ - æµ‹è¯•å‡†ç¡®ç‡)', va='bottom')
    ax2.grid(True)
    
    # 3. æœ€ä½³åˆ†ç±»å™¨çš„æ··æ·†çŸ©é˜µçƒ­å›¾
    ax3 = axes[1, 0]
    best_name = max(results.items(), key=lambda x: x[1]['test_accuracy'])[0]
    best_result = results[best_name]
    cm = best_result['confusion_matrix']
    
    im = ax3.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax3.set_title(f'æœ€ä½³åˆ†ç±»å™¨: {best_name}\næ··æ·†çŸ©é˜µ')
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im, ax=ax3)
    
    # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax3.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    
    ax3.set_xticks(range(len(data['target_names'])))
    ax3.set_yticks(range(len(data['target_names'])))
    ax3.set_xticklabels(data['target_names'])
    ax3.set_yticklabels(data['target_names'])
    ax3.set_ylabel('çœŸå®æ ‡ç­¾')
    ax3.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    
    # 4. ç‰¹å¾é‡è¦æ€§ï¼ˆä»…é€‚ç”¨äºæ ‘æ¨¡å‹å’Œçº¿æ€§æ¨¡å‹ï¼‰
    ax4 = axes[1, 1]
    
    # å°è¯•è·å–ç‰¹å¾é‡è¦æ€§
    feature_importance_data = []
    importance_labels = []
    
    for name, result in results.items():
        clf = result['classifier']
        
        if hasattr(clf, 'coef_'):
            # çº¿æ€§æ¨¡å‹ç³»æ•°
            if len(clf.coef_.shape) == 2:  # å¤šåˆ†ç±»æƒ…å†µ
                importance = np.abs(clf.coef_).mean(axis=0)
            else:
                importance = np.abs(clf.coef_[0])
            feature_importance_data.append(importance)
            importance_labels.append(name)
        elif hasattr(clf, 'feature_importances_'):
            # æ ‘æ¨¡å‹ç‰¹å¾é‡è¦æ€§
            feature_importance_data.append(clf.feature_importances_)
            importance_labels.append(name)
    
    if feature_importance_data:
        importance_matrix = np.array(feature_importance_data)
        x = np.arange(len(data['feature_names']))
        width = 0.8 / len(importance_labels)
        
        for i, (label, importance) in enumerate(zip(importance_labels, feature_importance_data)):
            offset = width * (i - len(importance_labels) / 2)
            ax4.bar(x + offset, importance, width, label=label, alpha=0.7)
        
        ax4.set_xlabel('ç‰¹å¾')
        ax4.set_ylabel('é‡è¦æ€§')
        ax4.set_title('ç‰¹å¾é‡è¦æ€§æ¯”è¾ƒ')
        ax4.set_xticks(x)
        ax4.set_xticklabels(data['feature_names'])
        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'ç‰¹å¾é‡è¦æ€§ä¸å¯ç”¨\n(ä»…éƒ¨åˆ†æ¨¡å‹æ”¯æŒ)', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('ç‰¹å¾é‡è¦æ€§')
    
    plt.tight_layout()
    plt.savefig('classifier_comparison_performance.png', dpi=150, bbox_inches='tight')
    plt.show()

def print_detailed_report(results, data):
    """æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("è¯¦ç»†æ€§èƒ½æŠ¥å‘Š")
    print("="*60)
    
    # æ‰¾åˆ°æœ€ä½³åˆ†ç±»å™¨
    best_name = max(results.items(), key=lambda x: x[1]['test_accuracy'])[0]
    best_result = results[best_name]
    
    print(f"\nğŸ† æœ€ä½³åˆ†ç±»å™¨: {best_name}")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_result['test_accuracy']:.4f}")
    print(f"   è®­ç»ƒå‡†ç¡®ç‡: {best_result['train_accuracy']:.4f}")
    
    print("\nğŸ“Š æ··æ·†çŸ©é˜µ:")
    print(best_result['confusion_matrix'])
    
    print("\nğŸ“ˆ æ‰€æœ‰åˆ†ç±»å™¨æ’å:")
    sorted_results = sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)
    for rank, (name, result) in enumerate(sorted_results, 1):
        print(f"   {rank}. {name}: {result['test_accuracy']:.4f} "
              f"(è®­ç»ƒ: {result['train_accuracy']:.4f}, "
              f"è¿‡æ‹Ÿåˆ: {result['train_accuracy'] - result['test_accuracy']:.4f})")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ¯”è¾ƒä¸åŒåˆ†ç±»å™¨åœ¨ä¸‰åˆ†ç±»/ä¸¤ä¸ªç‰¹å¾ä¸Šçš„è¡¨ç°")
    print("="*60)
    
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
    print("\nğŸ“Š æ­¥éª¤1: åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
    data = load_and_prepare_data()
    
    # 2. åˆ›å»ºåˆ†ç±»å™¨
    print("\nğŸ”§ æ­¥éª¤2: åˆ›å»ºåˆ†ç±»å™¨...")
    classifiers = create_classifiers()
    
    # 3. è®­ç»ƒå’Œè¯„ä¼°
    print("\nâš™ï¸ æ­¥éª¤3: è®­ç»ƒå’Œè¯„ä¼°åˆ†ç±»å™¨...")
    results = train_and_evaluate_classifiers(classifiers, data)
    
    # 4. å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
    print("\nğŸ¨ æ­¥éª¤4: ç»˜åˆ¶å†³ç­–è¾¹ç•Œ...")
    plot_decision_boundaries(results, data)
    
    # 5. å¯è§†åŒ–æ€§èƒ½æ¯”è¾ƒ
    print("\nğŸ“ˆ æ­¥éª¤5: ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾...")
    plot_performance_comparison(results, data)
    
    # 6. æ‰“å°è¯¦ç»†æŠ¥å‘Š
    print_detailed_report(results, data)
    
    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("å·²ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:")
    print("  - classifier_comparison_decision_boundaries.png")
    print("  - classifier_comparison_performance.png")
    print("="*60)

if __name__ == "__main__":
    main()